{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import sys\n",
    "from abc import abstractmethod\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b1\n",
      "Loaded pretrained weights for efficientnet-b2\n",
      "Loaded pretrained weights for efficientnet-b3\n",
      "Loaded pretrained weights for efficientnet-b4\n",
      "Loaded pretrained weights for efficientnet-b5\n",
      "Loaded pretrained weights for efficientnet-b6\n",
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "# AUTO = tf.data.experimental.AUTOTUNE\n",
    "# IMAGE_SIZE = [512, 512]\n",
    "# EPOCHS = 2000\n",
    "# BATCH_SIZE_PER_TPU = 8\n",
    "# BATCH_SIZE = BATCH_SIZE_PER_TPU * strategy.num_replicas_in_sync\n",
    "# FOLDERNAME = \"v2clean_sample\"\n",
    "NUM_CLASSES = 81313\n",
    "EMB_SIZE = 512\n",
    "EFNS = [\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b0\", num_classes=EMB_SIZE),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b1\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b2\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b3\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b4\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b5\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b6\"),\n",
    "    EfficientNet.from_pretrained(\"efficientnet-b7\", num_classes=EMB_SIZE),\n",
    "]\n",
    "EFF_VER = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1          [-1, 3, 513, 513]               0\n",
      "Conv2dStaticSamePadding-2         [-1, 32, 256, 256]             864\n",
      "       BatchNorm2d-3         [-1, 32, 256, 256]              64\n",
      "MemoryEfficientSwish-4         [-1, 32, 256, 256]               0\n",
      "         ZeroPad2d-5         [-1, 32, 258, 258]               0\n",
      "Conv2dStaticSamePadding-6         [-1, 32, 256, 256]             288\n",
      "       BatchNorm2d-7         [-1, 32, 256, 256]              64\n",
      "MemoryEfficientSwish-8         [-1, 32, 256, 256]               0\n",
      "          Identity-9             [-1, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n",
      "         Identity-12              [-1, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n",
      "         Identity-14         [-1, 32, 256, 256]               0\n",
      "Conv2dStaticSamePadding-15         [-1, 16, 256, 256]             512\n",
      "      BatchNorm2d-16         [-1, 16, 256, 256]              32\n",
      "      MBConvBlock-17         [-1, 16, 256, 256]               0\n",
      "         Identity-18         [-1, 16, 256, 256]               0\n",
      "Conv2dStaticSamePadding-19         [-1, 96, 256, 256]           1,536\n",
      "      BatchNorm2d-20         [-1, 96, 256, 256]             192\n",
      "MemoryEfficientSwish-21         [-1, 96, 256, 256]               0\n",
      "        ZeroPad2d-22         [-1, 96, 257, 257]               0\n",
      "Conv2dStaticSamePadding-23         [-1, 96, 128, 128]             864\n",
      "      BatchNorm2d-24         [-1, 96, 128, 128]             192\n",
      "MemoryEfficientSwish-25         [-1, 96, 128, 128]               0\n",
      "         Identity-26             [-1, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27              [-1, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28              [-1, 4, 1, 1]               0\n",
      "         Identity-29              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30             [-1, 96, 1, 1]             480\n",
      "         Identity-31         [-1, 96, 128, 128]               0\n",
      "Conv2dStaticSamePadding-32         [-1, 24, 128, 128]           2,304\n",
      "      BatchNorm2d-33         [-1, 24, 128, 128]              48\n",
      "      MBConvBlock-34         [-1, 24, 128, 128]               0\n",
      "         Identity-35         [-1, 24, 128, 128]               0\n",
      "Conv2dStaticSamePadding-36        [-1, 144, 128, 128]           3,456\n",
      "      BatchNorm2d-37        [-1, 144, 128, 128]             288\n",
      "MemoryEfficientSwish-38        [-1, 144, 128, 128]               0\n",
      "        ZeroPad2d-39        [-1, 144, 130, 130]               0\n",
      "Conv2dStaticSamePadding-40        [-1, 144, 128, 128]           1,296\n",
      "      BatchNorm2d-41        [-1, 144, 128, 128]             288\n",
      "MemoryEfficientSwish-42        [-1, 144, 128, 128]               0\n",
      "         Identity-43            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45              [-1, 6, 1, 1]               0\n",
      "         Identity-46              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47            [-1, 144, 1, 1]           1,008\n",
      "         Identity-48        [-1, 144, 128, 128]               0\n",
      "Conv2dStaticSamePadding-49         [-1, 24, 128, 128]           3,456\n",
      "      BatchNorm2d-50         [-1, 24, 128, 128]              48\n",
      "      MBConvBlock-51         [-1, 24, 128, 128]               0\n",
      "         Identity-52         [-1, 24, 128, 128]               0\n",
      "Conv2dStaticSamePadding-53        [-1, 144, 128, 128]           3,456\n",
      "      BatchNorm2d-54        [-1, 144, 128, 128]             288\n",
      "MemoryEfficientSwish-55        [-1, 144, 128, 128]               0\n",
      "        ZeroPad2d-56        [-1, 144, 131, 131]               0\n",
      "Conv2dStaticSamePadding-57          [-1, 144, 64, 64]           3,600\n",
      "      BatchNorm2d-58          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-59          [-1, 144, 64, 64]               0\n",
      "         Identity-60            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62              [-1, 6, 1, 1]               0\n",
      "         Identity-63              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64            [-1, 144, 1, 1]           1,008\n",
      "         Identity-65          [-1, 144, 64, 64]               0\n",
      "Conv2dStaticSamePadding-66           [-1, 40, 64, 64]           5,760\n",
      "      BatchNorm2d-67           [-1, 40, 64, 64]              80\n",
      "      MBConvBlock-68           [-1, 40, 64, 64]               0\n",
      "         Identity-69           [-1, 40, 64, 64]               0\n",
      "Conv2dStaticSamePadding-70          [-1, 240, 64, 64]           9,600\n",
      "      BatchNorm2d-71          [-1, 240, 64, 64]             480\n",
      "MemoryEfficientSwish-72          [-1, 240, 64, 64]               0\n",
      "        ZeroPad2d-73          [-1, 240, 68, 68]               0\n",
      "Conv2dStaticSamePadding-74          [-1, 240, 64, 64]           6,000\n",
      "      BatchNorm2d-75          [-1, 240, 64, 64]             480\n",
      "MemoryEfficientSwish-76          [-1, 240, 64, 64]               0\n",
      "         Identity-77            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79             [-1, 10, 1, 1]               0\n",
      "         Identity-80             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81            [-1, 240, 1, 1]           2,640\n",
      "         Identity-82          [-1, 240, 64, 64]               0\n",
      "Conv2dStaticSamePadding-83           [-1, 40, 64, 64]           9,600\n",
      "      BatchNorm2d-84           [-1, 40, 64, 64]              80\n",
      "      MBConvBlock-85           [-1, 40, 64, 64]               0\n",
      "         Identity-86           [-1, 40, 64, 64]               0\n",
      "Conv2dStaticSamePadding-87          [-1, 240, 64, 64]           9,600\n",
      "      BatchNorm2d-88          [-1, 240, 64, 64]             480\n",
      "MemoryEfficientSwish-89          [-1, 240, 64, 64]               0\n",
      "        ZeroPad2d-90          [-1, 240, 65, 65]               0\n",
      "Conv2dStaticSamePadding-91          [-1, 240, 32, 32]           2,160\n",
      "      BatchNorm2d-92          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-93          [-1, 240, 32, 32]               0\n",
      "         Identity-94            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-95             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-96             [-1, 10, 1, 1]               0\n",
      "         Identity-97             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-98            [-1, 240, 1, 1]           2,640\n",
      "         Identity-99          [-1, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-100           [-1, 80, 32, 32]          19,200\n",
      "     BatchNorm2d-101           [-1, 80, 32, 32]             160\n",
      "     MBConvBlock-102           [-1, 80, 32, 32]               0\n",
      "        Identity-103           [-1, 80, 32, 32]               0\n",
      "Conv2dStaticSamePadding-104          [-1, 480, 32, 32]          38,400\n",
      "     BatchNorm2d-105          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-106          [-1, 480, 32, 32]               0\n",
      "       ZeroPad2d-107          [-1, 480, 34, 34]               0\n",
      "Conv2dStaticSamePadding-108          [-1, 480, 32, 32]           4,320\n",
      "     BatchNorm2d-109          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-110          [-1, 480, 32, 32]               0\n",
      "        Identity-111            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-112             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-113             [-1, 20, 1, 1]               0\n",
      "        Identity-114             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-115            [-1, 480, 1, 1]          10,080\n",
      "        Identity-116          [-1, 480, 32, 32]               0\n",
      "Conv2dStaticSamePadding-117           [-1, 80, 32, 32]          38,400\n",
      "     BatchNorm2d-118           [-1, 80, 32, 32]             160\n",
      "     MBConvBlock-119           [-1, 80, 32, 32]               0\n",
      "        Identity-120           [-1, 80, 32, 32]               0\n",
      "Conv2dStaticSamePadding-121          [-1, 480, 32, 32]          38,400\n",
      "     BatchNorm2d-122          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-123          [-1, 480, 32, 32]               0\n",
      "       ZeroPad2d-124          [-1, 480, 34, 34]               0\n",
      "Conv2dStaticSamePadding-125          [-1, 480, 32, 32]           4,320\n",
      "     BatchNorm2d-126          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-127          [-1, 480, 32, 32]               0\n",
      "        Identity-128            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-129             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-130             [-1, 20, 1, 1]               0\n",
      "        Identity-131             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-132            [-1, 480, 1, 1]          10,080\n",
      "        Identity-133          [-1, 480, 32, 32]               0\n",
      "Conv2dStaticSamePadding-134           [-1, 80, 32, 32]          38,400\n",
      "     BatchNorm2d-135           [-1, 80, 32, 32]             160\n",
      "     MBConvBlock-136           [-1, 80, 32, 32]               0\n",
      "        Identity-137           [-1, 80, 32, 32]               0\n",
      "Conv2dStaticSamePadding-138          [-1, 480, 32, 32]          38,400\n",
      "     BatchNorm2d-139          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-140          [-1, 480, 32, 32]               0\n",
      "       ZeroPad2d-141          [-1, 480, 36, 36]               0\n",
      "Conv2dStaticSamePadding-142          [-1, 480, 32, 32]          12,000\n",
      "     BatchNorm2d-143          [-1, 480, 32, 32]             960\n",
      "MemoryEfficientSwish-144          [-1, 480, 32, 32]               0\n",
      "        Identity-145            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-146             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-147             [-1, 20, 1, 1]               0\n",
      "        Identity-148             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-149            [-1, 480, 1, 1]          10,080\n",
      "        Identity-150          [-1, 480, 32, 32]               0\n",
      "Conv2dStaticSamePadding-151          [-1, 112, 32, 32]          53,760\n",
      "     BatchNorm2d-152          [-1, 112, 32, 32]             224\n",
      "     MBConvBlock-153          [-1, 112, 32, 32]               0\n",
      "        Identity-154          [-1, 112, 32, 32]               0\n",
      "Conv2dStaticSamePadding-155          [-1, 672, 32, 32]          75,264\n",
      "     BatchNorm2d-156          [-1, 672, 32, 32]           1,344\n",
      "MemoryEfficientSwish-157          [-1, 672, 32, 32]               0\n",
      "       ZeroPad2d-158          [-1, 672, 36, 36]               0\n",
      "Conv2dStaticSamePadding-159          [-1, 672, 32, 32]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 32, 32]           1,344\n",
      "MemoryEfficientSwish-161          [-1, 672, 32, 32]               0\n",
      "        Identity-162            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-163             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-164             [-1, 28, 1, 1]               0\n",
      "        Identity-165             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-166            [-1, 672, 1, 1]          19,488\n",
      "        Identity-167          [-1, 672, 32, 32]               0\n",
      "Conv2dStaticSamePadding-168          [-1, 112, 32, 32]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 32, 32]             224\n",
      "     MBConvBlock-170          [-1, 112, 32, 32]               0\n",
      "        Identity-171          [-1, 112, 32, 32]               0\n",
      "Conv2dStaticSamePadding-172          [-1, 672, 32, 32]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 32, 32]           1,344\n",
      "MemoryEfficientSwish-174          [-1, 672, 32, 32]               0\n",
      "       ZeroPad2d-175          [-1, 672, 36, 36]               0\n",
      "Conv2dStaticSamePadding-176          [-1, 672, 32, 32]          16,800\n",
      "     BatchNorm2d-177          [-1, 672, 32, 32]           1,344\n",
      "MemoryEfficientSwish-178          [-1, 672, 32, 32]               0\n",
      "        Identity-179            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-180             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-181             [-1, 28, 1, 1]               0\n",
      "        Identity-182             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-183            [-1, 672, 1, 1]          19,488\n",
      "        Identity-184          [-1, 672, 32, 32]               0\n",
      "Conv2dStaticSamePadding-185          [-1, 112, 32, 32]          75,264\n",
      "     BatchNorm2d-186          [-1, 112, 32, 32]             224\n",
      "     MBConvBlock-187          [-1, 112, 32, 32]               0\n",
      "        Identity-188          [-1, 112, 32, 32]               0\n",
      "Conv2dStaticSamePadding-189          [-1, 672, 32, 32]          75,264\n",
      "     BatchNorm2d-190          [-1, 672, 32, 32]           1,344\n",
      "MemoryEfficientSwish-191          [-1, 672, 32, 32]               0\n",
      "       ZeroPad2d-192          [-1, 672, 35, 35]               0\n",
      "Conv2dStaticSamePadding-193          [-1, 672, 16, 16]          16,800\n",
      "     BatchNorm2d-194          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-195          [-1, 672, 16, 16]               0\n",
      "        Identity-196            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-197             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-198             [-1, 28, 1, 1]               0\n",
      "        Identity-199             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-200            [-1, 672, 1, 1]          19,488\n",
      "        Identity-201          [-1, 672, 16, 16]               0\n",
      "Conv2dStaticSamePadding-202          [-1, 192, 16, 16]         129,024\n",
      "     BatchNorm2d-203          [-1, 192, 16, 16]             384\n",
      "     MBConvBlock-204          [-1, 192, 16, 16]               0\n",
      "        Identity-205          [-1, 192, 16, 16]               0\n",
      "Conv2dStaticSamePadding-206         [-1, 1152, 16, 16]         221,184\n",
      "     BatchNorm2d-207         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-208         [-1, 1152, 16, 16]               0\n",
      "       ZeroPad2d-209         [-1, 1152, 20, 20]               0\n",
      "Conv2dStaticSamePadding-210         [-1, 1152, 16, 16]          28,800\n",
      "     BatchNorm2d-211         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-212         [-1, 1152, 16, 16]               0\n",
      "        Identity-213           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-214             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-215             [-1, 48, 1, 1]               0\n",
      "        Identity-216             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-217           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-218         [-1, 1152, 16, 16]               0\n",
      "Conv2dStaticSamePadding-219          [-1, 192, 16, 16]         221,184\n",
      "     BatchNorm2d-220          [-1, 192, 16, 16]             384\n",
      "     MBConvBlock-221          [-1, 192, 16, 16]               0\n",
      "        Identity-222          [-1, 192, 16, 16]               0\n",
      "Conv2dStaticSamePadding-223         [-1, 1152, 16, 16]         221,184\n",
      "     BatchNorm2d-224         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-225         [-1, 1152, 16, 16]               0\n",
      "       ZeroPad2d-226         [-1, 1152, 20, 20]               0\n",
      "Conv2dStaticSamePadding-227         [-1, 1152, 16, 16]          28,800\n",
      "     BatchNorm2d-228         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-229         [-1, 1152, 16, 16]               0\n",
      "        Identity-230           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-231             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-232             [-1, 48, 1, 1]               0\n",
      "        Identity-233             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-234           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-235         [-1, 1152, 16, 16]               0\n",
      "Conv2dStaticSamePadding-236          [-1, 192, 16, 16]         221,184\n",
      "     BatchNorm2d-237          [-1, 192, 16, 16]             384\n",
      "     MBConvBlock-238          [-1, 192, 16, 16]               0\n",
      "        Identity-239          [-1, 192, 16, 16]               0\n",
      "Conv2dStaticSamePadding-240         [-1, 1152, 16, 16]         221,184\n",
      "     BatchNorm2d-241         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-242         [-1, 1152, 16, 16]               0\n",
      "       ZeroPad2d-243         [-1, 1152, 20, 20]               0\n",
      "Conv2dStaticSamePadding-244         [-1, 1152, 16, 16]          28,800\n",
      "     BatchNorm2d-245         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-246         [-1, 1152, 16, 16]               0\n",
      "        Identity-247           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-248             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-249             [-1, 48, 1, 1]               0\n",
      "        Identity-250             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-251           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-252         [-1, 1152, 16, 16]               0\n",
      "Conv2dStaticSamePadding-253          [-1, 192, 16, 16]         221,184\n",
      "     BatchNorm2d-254          [-1, 192, 16, 16]             384\n",
      "     MBConvBlock-255          [-1, 192, 16, 16]               0\n",
      "        Identity-256          [-1, 192, 16, 16]               0\n",
      "Conv2dStaticSamePadding-257         [-1, 1152, 16, 16]         221,184\n",
      "     BatchNorm2d-258         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-259         [-1, 1152, 16, 16]               0\n",
      "       ZeroPad2d-260         [-1, 1152, 18, 18]               0\n",
      "Conv2dStaticSamePadding-261         [-1, 1152, 16, 16]          10,368\n",
      "     BatchNorm2d-262         [-1, 1152, 16, 16]           2,304\n",
      "MemoryEfficientSwish-263         [-1, 1152, 16, 16]               0\n",
      "        Identity-264           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-265             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-266             [-1, 48, 1, 1]               0\n",
      "        Identity-267             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-268           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-269         [-1, 1152, 16, 16]               0\n",
      "Conv2dStaticSamePadding-270          [-1, 320, 16, 16]         368,640\n",
      "     BatchNorm2d-271          [-1, 320, 16, 16]             640\n",
      "     MBConvBlock-272          [-1, 320, 16, 16]               0\n",
      "        Identity-273          [-1, 320, 16, 16]               0\n",
      "Conv2dStaticSamePadding-274         [-1, 1280, 16, 16]         409,600\n",
      "     BatchNorm2d-275         [-1, 1280, 16, 16]           2,560\n",
      "MemoryEfficientSwish-276         [-1, 1280, 16, 16]               0\n",
      "AdaptiveAvgPool2d-277           [-1, 1280, 1, 1]               0\n",
      "         Dropout-278                 [-1, 1280]               0\n",
      "          Linear-279                  [-1, 512]         655,872\n",
      "================================================================\n",
      "Total params: 4,663,420\n",
      "Trainable params: 4,663,420\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 1091.37\n",
      "Params size (MB): 17.79\n",
      "Estimated Total Size (MB): 1112.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(EFNS[EFF_VER], (3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngularMarginHead(nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_size: int, nrof_classes: int, s: int, m: float, clip: Optional[bool] = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.min_allowed = math.cos(math.pi - m)\n",
    "        self.clip = clip\n",
    "\n",
    "        self._cosine = None\n",
    "        self.sine = None\n",
    "\n",
    "        self.weight = nn.parameter.Parameter(torch.Tensor(nrof_classes, feature_size))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    @property\n",
    "    def cosine(self):\n",
    "        return self._cosine\n",
    "\n",
    "    @cosine.setter\n",
    "    def cosine(self, value):\n",
    "        self._cosine = value\n",
    "        self.sine = torch.sqrt(1 - self.cosine ** 2)\n",
    "\n",
    "    @abstractmethod\n",
    "    def positive_cosine_similarity_modulator(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Positive cosine similarity modulator\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def negative_cosine_similarity_modulator(\n",
    "        self, cosine_after_positive_modulator: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Negative cosine similarity modulator\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, features: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        self.cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        if self.clip:\n",
    "            self.cosine = torch.clip(self.cosine, self.min_allowed, 0.99)\n",
    "\n",
    "        one_hot = torch.zeros_like(self.cosine).to(y.device)\n",
    "        one_hot.scatter_(1, y.view(-1, 1).long(), 1)\n",
    "\n",
    "        cosine_after_positive_modulator = self.positive_cosine_similarity_modulator()\n",
    "        cosine_after_negative_modulator = self.negative_cosine_similarity_modulator(\n",
    "            cosine_after_positive_modulator\n",
    "        )\n",
    "\n",
    "        output = torch.where(\n",
    "            one_hot == 1, cosine_after_positive_modulator, cosine_after_negative_modulator\n",
    "        )\n",
    "        return self.s * output\n",
    "\n",
    "\n",
    "class ArcFace(AngularMarginHead):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_size: int,\n",
    "        nrof_classes: int,\n",
    "        s: Optional[int] = 64,\n",
    "        m: Optional[float] = 0.5,\n",
    "        clip: Optional[bool] = True,\n",
    "    ) -> None:\n",
    "        super().__init__(feature_size, nrof_classes, s, m, clip)\n",
    "\n",
    "    def positive_cosine_similarity_modulator(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Positive cosine modulator for ArcFace is: T(cos(theta)) = cos(theta + m),\n",
    "        if we expand this expression it becomes: cos(theta)*cos(m) - sin(theta)*sin(m)\n",
    "        \"\"\"\n",
    "        return self.cosine * self.cos_m - self.sine * self.sin_m\n",
    "\n",
    "    def negative_cosine_similarity_modulator(\n",
    "        self, cosine_after_positive_modulator: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        There is no modulation for negative cosine similarity in ArcFace\n",
    "        \"\"\"\n",
    "        return self.cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetBackbone(nn.Module):\n",
    "    def __init__(self, feature_size: int):\n",
    "        super(EfficientNetBackbone, self).__init__()\n",
    "\n",
    "        self.efficientNet = EFNS[EFF_VER]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientNet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger():\n",
    "    logger = logging.getLogger(\"train\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if len(logger.handlers) == 0:\n",
    "        formatter = logging.Formatter(\"%(asctime)s | %(message)s\")\n",
    "        ch = logging.StreamHandler(stream=sys.stdout)\n",
    "        ch.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        fh = logging.handlers.WatchedFileHandler(\"train.log\")\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "\n",
    "class SummaryWriter:\n",
    "    def __init__(self, nb_epochs: int, nb_batchs: int) -> None:\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_batchs = nb_batchs\n",
    "        self.epoch = 0\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def __call__(self, mode: str, i_batch: int, metrics: Dict[str, float]) -> None:\n",
    "        summary = f\"{mode.title()} Epoch {self.epoch}/{self.nb_epochs} | Batch {i_batch}/{self.nb_batchs} | \"\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            summary += f\"{metric_name.title()} {metric_value:.2f} | \"\n",
    "        logger.info(summary[:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def calculate_accuracy(logits: torch.Tensor, y: torch.Tensor) -> float:\n",
    "    y_pred = torch.argmax(logits, dim=1)\n",
    "    return torch.mean((y_pred == y).float()).item()\n",
    "\n",
    "\n",
    "def pass_epoch(\n",
    "    loader: DataLoader,\n",
    "    backbone: nn.Module,\n",
    "    angular_margin: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    summary_writer: SummaryWriter,\n",
    "    log_interval: int,\n",
    "    device: str,\n",
    ") -> Tuple[float]:\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    with torch.set_grad_enabled(backbone.training):\n",
    "        for i_batch, (x, y) in enumerate(loader):\n",
    "            logging.info(i_batch)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logging.info(\"Computing embeddings\")\n",
    "            embeddings = backbone(x)\n",
    "            logits = angular_margin(embeddings, y)\n",
    "\n",
    "            logging.info(\"Computing loss\")\n",
    "            print(logits.shape)\n",
    "            print(y.shape)\n",
    "            loss_batch = loss_fn(logits, y)\n",
    "            acc_batch = calculate_accuracy(logits, y)\n",
    "\n",
    "            logging.info(\"Updating weights\")\n",
    "            if backbone.training:\n",
    "                optimizer.zero_grad()\n",
    "                loss_batch.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_batch = loss_batch.item()\n",
    "            if i_batch % log_interval == 0:\n",
    "                mode = \"train\" if backbone.training else \"validation\"\n",
    "                summary_writer(mode, i_batch, {\"loss\": loss_batch, \"acc\": acc_batch})\n",
    "\n",
    "            loss += loss_batch\n",
    "            acc += acc_batch\n",
    "\n",
    "    loss /= i_batch + 1\n",
    "    acc /= i_batch + 1\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    angular_margin: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    feature_size: int,\n",
    "    lr: float,\n",
    "    nb_epochs: int,\n",
    "    log_interval: int,\n",
    "    early_stop_after: int,\n",
    "    device: str,\n",
    ") -> nn.Module:\n",
    "\n",
    "    backbone = EfficientNetBackbone(feature_size)\n",
    "    backbone.to(device)\n",
    "    angular_margin.to(device)\n",
    "\n",
    "    backbone_state_dict = backbone.state_dict()\n",
    "    head_state_dict = angular_margin.state_dict()\n",
    "\n",
    "    optimizer = optim.Adam(list(backbone.parameters()) + list(angular_margin.parameters()), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "    summary_writer = SummaryWriter(nb_epochs, len(train_loader))\n",
    "\n",
    "    logging.info(\"Starting\")\n",
    "\n",
    "    min_loss = math.inf\n",
    "    for epoch in range(nb_epochs):\n",
    "        summary_writer.set_epoch(epoch + 1)\n",
    "\n",
    "        backbone.train()\n",
    "        angular_margin.train()\n",
    "\n",
    "        logging.info(\"Passing epoch\")\n",
    "        loss, acc = pass_epoch(\n",
    "            train_loader,\n",
    "            backbone,\n",
    "            angular_margin,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            summary_writer,\n",
    "            log_interval,\n",
    "            device,\n",
    "        )\n",
    "        logger.info(f\"Train Epoch Loss: {loss:.2f} | Accuracy: {acc:.2f}\\n\")\n",
    "\n",
    "        backbone.eval()\n",
    "        angular_margin.eval()\n",
    "        loss, acc = pass_epoch(\n",
    "            validation_loader,\n",
    "            backbone,\n",
    "            angular_margin,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            summary_writer,\n",
    "            log_interval,\n",
    "            device,\n",
    "        )\n",
    "        acc = round(acc, 2)\n",
    "        logger.info(f\"Validation Epoch Loss: {loss:.2f} | Accuracy: {acc}\\n\")\n",
    "\n",
    "        if loss > min_loss:\n",
    "            min_loss = loss\n",
    "            early_stop_counter = 0\n",
    "            backbone_state_dict = backbone.state_dict()\n",
    "            head_state_dict = angular_margin.state_dict()\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter == early_stop_after:\n",
    "            break\n",
    "\n",
    "        scheduler.step(loss)\n",
    "\n",
    "    backbone.load_state_dict(backbone_state_dict)\n",
    "    angular_margin.load_state_dict(head_state_dict)\n",
    "    return backbone, angular_margin, min_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(\n",
    "    experiment_name: str, test_loader: DataLoader, backbone: nn.Module, device: str\n",
    ") -> Tuple[np.ndarray]:\n",
    "    logger.info(f\"Extracting embeddings with the model with: {experiment_name}\")\n",
    "    log_interval = len(test_loader) // 5\n",
    "    test_embeddings = []\n",
    "\n",
    "    backbone.eval()\n",
    "    for i_batch, x in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        test_embeddings.append(backbone(x))\n",
    "\n",
    "        if i_batch % log_interval == 0:\n",
    "            logger.info(f\"Extracting embedings Batch {i_batch}/{len(test_loader)}\")\n",
    "\n",
    "    return torch.cat(test_embeddings).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-17 00:52:08,544 | Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Head parameters\n",
    "m = 0.5  # For all\n",
    "s = 64  # For all\n",
    "alpha = 0.99  # For CurricularFace\n",
    "t = 1.2  # For MV-Arc-Softmax\n",
    "\n",
    "nb_epochs = 50\n",
    "batch_size = 256\n",
    "lr = 5e-4\n",
    "loss_fn_to_use = \"softmax\"\n",
    "early_stop_after = 11\n",
    "feature_size = 512  # Embeddings size\n",
    "nb_classes = 81313  # The clean_train dataset has 81313 classes\n",
    "log_interval = 50\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transformations = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((512,512)),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
    "            std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(folder, image_id):\n",
    "    return os.path.join(folder, f\"{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GoogleLandmarkDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        landmarks = sorted(list(set(self.img_labels[\"landmark_id\"])))\n",
    "        self.landmark_id_to_label = {landmarks[i]: i for i in range(len(landmarks))}\n",
    "        self.img_dir = img_dir\n",
    "        self.num_classes = len(landmarks)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = get_path(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = cv.imread(img_path)\n",
    "        label = self.landmark_id_to_label[self.img_labels.iloc[idx, 1]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GoogleLandmarkDataset(\n",
    "    img_dir=\"./data/train\",\n",
    "    annotations_file=\"./data/train.csv\",\n",
    "    transform=transformations,\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "nb_classes = train_dataset.num_classes\n",
    "\n",
    "train_dataset, validation_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# test_dataset = GoogleLandmarkDataset(\n",
    "#     img_dir=\"../data/test\", annotations_file=\"../data/test.csv\", transform=transformations\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=16,\n",
    "#     drop_last=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 349])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting\n",
      "INFO:root:Passing epoch\n",
      "INFO:root:0\n",
      "INFO:root:Computing embeddings\n",
      "INFO:root:Computing loss\n",
      "INFO:root:Updating weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 349])\n",
      "torch.Size([16])\n",
      "2022-01-17 01:22:50,503 | Train Epoch 1/10 | Batch 0/23 | Loss 39.86 | Acc 0.00 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:train:Train Epoch 1/10 | Batch 0/23 | Loss 39.86 | Acc 0.00 \n",
      "INFO:root:1\n",
      "INFO:root:Computing embeddings\n",
      "INFO:root:Computing loss\n",
      "INFO:root:Updating weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 349])\n",
      "torch.Size([16])\n",
      "2022-01-17 01:27:24,003 | Train Epoch 1/10 | Batch 1/23 | Loss 42.75 | Acc 0.00 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:train:Train Epoch 1/10 | Batch 1/23 | Loss 42.75 | Acc 0.00 \n",
      "INFO:root:2\n",
      "INFO:root:Computing embeddings\n",
      "INFO:root:Computing loss\n",
      "INFO:root:Updating weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 349])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/78/6hdr_rfd7rv9l_2gc4sts6600000gn/T/ipykernel_88010/221360856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m backbone, head, acc = train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mArcFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/6hdr_rfd7rv9l_2gc4sts6600000gn/T/ipykernel_88010/1653577670.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, validation_loader, angular_margin, loss_fn, feature_size, lr, nb_epochs, log_interval, early_stop_after, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passing epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         loss, acc = pass_epoch(\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/78/6hdr_rfd7rv9l_2gc4sts6600000gn/T/ipykernel_88010/1653577670.py\u001b[0m in \u001b[0;36mpass_epoch\u001b[0;34m(loader, backbone, angular_margin, optimizer, loss_fn, summary_writer, log_interval, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBackwardCFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FunctionBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctionCtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_HookMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# The user should define either backward or vjp but never both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "backbone, head, acc = train(\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    ArcFace(feature_size, nb_classes, s, m),\n",
    "    nn.CrossEntropyLoss(),\n",
    "    feature_size,\n",
    "    lr,\n",
    "    10,\n",
    "    1,\n",
    "    early_stop_after,\n",
    "    device,\n",
    ")\n",
    "\n",
    "plt.figure(num=None, figsize=(15, 15), dpi=80, facecolor=\"w\", edgecolor=\"k\")\n",
    "plt.plot(head.ts)\n",
    "plt.title(\"t values during training in CurricularFace\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"t\")\n",
    "logger.info(60 * \"-\" + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading images from Google Landmark Dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images_000.tar and its md5sum...\n",
      "Downloading images_001.tar and its md5sum...\n",
      "Downloading images_002.tar and its md5sum...\n",
      "Downloading images_003.tar and its md5sum...\n",
      "Downloading images_004.tar and its md5sum...\n",
      "images_001.tar extracted!\n",
      "images_004.tar extracted!\n",
      "images_003.tar extracted!\n",
      "images_000.tar extracted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Transfering downloaded images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images_002.tar extracted!\n"
     ]
    }
   ],
   "source": [
    "from src.data.download import download_and_sort\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level = logging.INFO)\n",
    "    \n",
    "download_and_sort(begin=0, end=4, nb_landmarks=50000)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
