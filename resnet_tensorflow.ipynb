{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten\n",
    "import numpy as np\n",
    "\n",
    "base_model = ResNet50(weights=\"imagenet\")\n",
    "\n",
    "strategy = tf.distribute.get_strategy()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer(\"avg_pool\").output)\n",
    "\"\"\" \n",
    "flatten = Flatten()(model.output)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=flatten) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode our images\n",
    "def decode_image(image_data, output_size=None):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, output_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to read our test image and return image\n",
    "def read_image(image, output_size=None):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = decode_image(image, output_size)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    base_folder, output_size: Optional[Tuple[int]] = None, nb_images: Optional[int] = None\n",
    "):\n",
    "\n",
    "    images_paths = glob.glob(os.path.join(base_folder, \"*/*/*/*.jpg\"))\n",
    "    images_ids = [os.path.splitext(os.path.basename(image))[0] for image in images_paths][\n",
    "        :nb_images\n",
    "    ]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_paths[:nb_images]))\n",
    "    dataset = dataset.map(\n",
    "        lambda img_path: read_image(img_path, output_size), num_parallel_calls=AUTO\n",
    "    )\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "\n",
    "    return images_ids, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(\n",
    "    images_folder, target_filename, model, output_size=None, nb_images=None, preprocess_input=None\n",
    "):\n",
    "    print(\"Loading images\")\n",
    "    images_ids, dataset = load_dataset(images_folder, output_size=output_size, nb_images=nb_images)\n",
    "\n",
    "    print(\"Preprocessing images\")\n",
    "    if preprocess_input is not None:\n",
    "        dataset.map(preprocess_input, num_parallel_calls=AUTO)\n",
    "\n",
    "    print(\"Computing embeddings\")\n",
    "    features = model.predict(dataset)\n",
    "\n",
    "    features = normalize(features)\n",
    "\n",
    "    print(\"Saving embeddings\")\n",
    "    np.save(target_filename, features)\n",
    "\n",
    "    print(\"Saving_ids\")\n",
    "    with open(os.path.splitext(target_filename)[0] + \"_ids.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(images_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "83798\n",
      "Preprocessing images\n",
      "Computing embeddings\n",
      "Saving embeddings\n",
      "Saving_ids\n"
     ]
    }
   ],
   "source": [
    "encode_dataset(\n",
    "    \"../index\",\n",
    "    \"./data/index_features.npy\",\n",
    "    model,\n",
    "    preprocess_input=preprocess_input,\n",
    "    output_size=(224, 224),\n",
    "    nb_images=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n",
      "23516\n",
      "Preprocessing images\n",
      "Computing embeddings\n",
      "Saving embeddings\n",
      "Saving_ids\n"
     ]
    }
   ],
   "source": [
    "encode_dataset(\n",
    "    \"../test\",\n",
    "    \"./data/test_features.npy\",\n",
    "    model,\n",
    "    preprocess_input=preprocess_input,\n",
    "    output_size=(224, 224),\n",
    "    nb_images=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_neighbors(index_ids, query_ids, index_embeddings, query_embeddings):\n",
    "\n",
    "    similarities = cosine_similarity(query_embeddings, index_embeddings)\n",
    "\n",
    "    results = {\"id\": [], \"images\": []}\n",
    "\n",
    "    for i, query_id in enumerate(query_ids):\n",
    "        results[\"id\"].append(query_id)\n",
    "        results[\"images\"].append(\" \".join(index_ids[np.argsort(similarities[i])[-100:][::-1]]))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/index_features_ids.txt\", \"r\") as f:\n",
    "    index_ids = np.asarray(f.read().split(\"\\n\"))\n",
    "with open(\"./data/test_features_ids.txt\", \"r\") as f:\n",
    "    query_ids = np.asarray(f.read().split(\"\\n\"))\n",
    "\n",
    "index_embeddings = np.load(\"./data/index_features.npy\")\n",
    "query_embeddings = np.load(\"./data/test_features.npy\")\n",
    "\n",
    "results = find_best_neighbors(index_ids, query_ids, index_embeddings, query_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(folder, image_id):\n",
    "    return os.path.join(folder, f\"{image_id[0]}/{image_id[1]}/{image_id[2]}/{image_id}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 124\n",
    "\n",
    "fig, axs = plt.subplots(1, 6, figsize=(15, 8))\n",
    "\n",
    "img = image.load_img(get_path(\"../test\", results[\"id\"][sample]))\n",
    "axs[0].imshow(img)\n",
    "axs[0].set_axis_off()\n",
    "\n",
    "\n",
    "images_ids = results[\"images\"][sample].split(\" \")\n",
    "for i in range(5):\n",
    "    img = image.load_img(get_path(\"../index\", images_ids[i]))\n",
    "    axs[i+1].imshow(img)\n",
    "    axs[i+1].set_axis_off()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results).to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b6f1c707ceca385f6c4ca5d12e48b0572f24b4e594ad28169dc0d157f691da6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
